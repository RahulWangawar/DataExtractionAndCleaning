# Data Cleaning, Schema Normalisation, Data Processing and Data Ingestion for Big Data Using Spark

### ABSTRACT

    1. ETL, which stands for Extract, Transform, and Load, is the process data engineers use to
    extract data from different sources in different formats, transform the data into a usable and
    trusted resource, and load that data into the systems end-users can access and use downstream
    to solve business problems. 
    
    2. After extraction of uncleaned data from different sources, like email attachments, Kaggle, 
    and various other sources. 
    
    First step of the pipeline is to clean the data. 
    In the transformation part, certain irrelevant attributes were removed from the original data, 
    as well as some necessary attributes were generated from the original data. 
    Finally, this usable format of data is stored on HDFS for application use.
