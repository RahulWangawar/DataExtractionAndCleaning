{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab5037e-849f-444a-8734-8749e574bb03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Convert raw files into parquet file format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b16a9b2-8ea1-40cc-b799-abf634e97833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/07 12:12:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "===============\n",
      "AppName: Free time exercise\n",
      "Master: local[*]\n",
      "===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15219768"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, Row, ArrayType, StringType\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "master = 'local'\n",
    "appName = 'Free time exercise'\n",
    "\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
    "\n",
    "if spark.sparkContext:\n",
    "    print('===============')\n",
    "    print(f'AppName: {spark.sparkContext.appName}')\n",
    "    print(f'Master: {spark.sparkContext.master}')\n",
    "    print('===============')\n",
    "else:\n",
    "    print('Could not initialise pyspark session')\n",
    "\n",
    "# df= spark.read.csv(\"file:///home/ns22/Desktop/rationcard/batch_1_500000_entities_23_dec_2021_rationcard.csv\",header=True, sep=\",\")\n",
    "df= spark.read.csv(\"file:///home/ns22/Desktop/rationcard/\",header=True, sep=\",\")\n",
    "# df1 = df.coalesce(1)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5853a8e2-bfa4-4b5a-aeee-3811831a2235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76832416-3356-4638-96b5-639904dd5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 =  df.coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45b4a5d-9961-4d39-95b1-ad332899cdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3.8 Gb ration card csv files compressed to 1.3 Gb\n",
    "df1.write.parquet(\"file:///home/ns22/Desktop/test_ration_parquet_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b6714a8-5c1c-46f8-a0ad-4b243eca5d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = spark.read.parquet(\"file:///home/ns22/Desktop/test_ration_parquet_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0eb3a7c-fef1-498f-81e2-7c1aeecbabd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15219768"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b4b93c3-9cd1-42a1-a1ad-0e5b131d84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "salutations = [\"mr\",\"miss\",\"dr\",\"srimati\",\"md\"]\n",
    "import re\n",
    "num_punc = \"[^a-zA-z\\\\s.]\"\n",
    "def clean_name(name):\n",
    "    if name is not None:\n",
    "        clean_name = re.sub(num_punc,r\"\",name.strip().lower())\n",
    "        if len(clean_name)==0:\n",
    "            return None\n",
    "        elif clean_name[1]==\".\" and clean_name[3]==\".\":\n",
    "            return clean_aname.replace(\" \",\"_\").title()\n",
    "        elif clean_name.split(\".\",1)[0] in salutations:\n",
    "            only_name = clean_name.split(\".\",1)[1]\n",
    "            return only_name.replace(\" \",\"_\").title()\n",
    "        else:\n",
    "            return clean_name.replace(\" \",\"_\").title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "872de746-9a40-4f07-9cb9-5ca2d8ea72c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "name_udf  = udf(clean_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "978f4037-e13b-4a1a-be3d-a1173b1c2087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|                name|         father_name|        mother_name|\n",
      "+--------------------+--------------------+-------------------+\n",
      "| Sabah Mati Alcasoas|             Prittam|      Kaavya Kanvar|\n",
      "|  Rebia Raj Alcasoas|Jagdeep Raj Alcasoas|Araati Lata Bhomkar|\n",
      "| Debiprasad Alcasoas|  Rebia Raj Alcasoas|Sabah Mati Alcasoas|\n",
      "|      Payal Alcasoas|   Rebia Rajalcasoas|Sabah Mati Alcasoas|\n",
      "|  Sarmistha Maisuria|   Rookmani Maisuria|    Kirthana Prabha|\n",
      "|Sukhdev Thottuchalil|            Jism Sen|       Peyari Jeesm|\n",
      "|         Rohana Doke|     Uma Prasad Doke|     Kanvar Supriti|\n",
      "|       Katwa Parihar|        Baru Parihar|       Eeshani Vati|\n",
      "|       Amari Parihar|      Kaatwa Parihar|        Rohana Doke|\n",
      "|  Ramjee Dev Parihar|       Katwa Parihar|        Rohana Doke|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select('name',\n",
    " 'father_name',\n",
    " 'mother_name').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f2b659b-f6a2-4389-b664-2fa4e9b849ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.withColumn(\"udf_name\",name_udf(test_df[\"name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63743ad5-80c9-47f8-bbc3-4fe875e9d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.withColumn(\"udf_f_name\",name_udf(test_df[\"father_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1ba354a-3521-4336-9f12-de70393b9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.withColumn(\"udf_m_name\",name_udf(test_df[\"mother_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eeef8fd-b822-4cc1-aecb-1f8210335bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "|            udf_name|          udf_f_name|         udf_m_name|                name|         father_name|        mother_name|\n",
      "+--------------------+--------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "| Sabah_Mati_Alcasoas|             Prittam|      Kaavya_Kanvar| Sabah Mati Alcasoas|             Prittam|      Kaavya Kanvar|\n",
      "|  Rebia_Raj_Alcasoas|Jagdeep_Raj_Alcasoas|Araati_Lata_Bhomkar|  Rebia Raj Alcasoas|Jagdeep Raj Alcasoas|Araati Lata Bhomkar|\n",
      "| Debiprasad_Alcasoas|  Rebia_Raj_Alcasoas|Sabah_Mati_Alcasoas| Debiprasad Alcasoas|  Rebia Raj Alcasoas|Sabah Mati Alcasoas|\n",
      "|      Payal_Alcasoas|   Rebia_Rajalcasoas|Sabah_Mati_Alcasoas|      Payal Alcasoas|   Rebia Rajalcasoas|Sabah Mati Alcasoas|\n",
      "|  Sarmistha_Maisuria|   Rookmani_Maisuria|    Kirthana_Prabha|  Sarmistha Maisuria|   Rookmani Maisuria|    Kirthana Prabha|\n",
      "|Sukhdev_Thottuchalil|            Jism_Sen|       Peyari_Jeesm|Sukhdev Thottuchalil|            Jism Sen|       Peyari Jeesm|\n",
      "|         Rohana_Doke|     Uma_Prasad_Doke|     Kanvar_Supriti|         Rohana Doke|     Uma Prasad Doke|     Kanvar Supriti|\n",
      "|       Katwa_Parihar|        Baru_Parihar|       Eeshani_Vati|       Katwa Parihar|        Baru Parihar|       Eeshani Vati|\n",
      "|       Amari_Parihar|      Kaatwa_Parihar|        Rohana_Doke|       Amari Parihar|      Kaatwa Parihar|        Rohana Doke|\n",
      "|  Ramjee_Dev_Parihar|       Katwa_Parihar|        Rohana_Doke|  Ramjee Dev Parihar|       Katwa Parihar|        Rohana Doke|\n",
      "+--------------------+--------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 3.35 ms, sys: 529 Âµs, total: 3.88 ms\n",
      "Wall time: 172 ms\n"
     ]
    }
   ],
   "source": [
    "%time test_df.select(\"udf_name\",\"udf_f_name\",\"udf_m_name\",\"name\",\"father_name\",\"mother_name\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75e11eb5-9ac1-409d-9323-6b58204fdf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.write.format(\"avro\").save(\"file:///home/ns22/Desktop/test_ration_avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e121e2bf-e198-420d-9516-2413cd04dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "from pyspark.sql import functions as F\n",
    "renamed_df = test_df.select([F.col(col).alias(col.replace('_', ' ')) for col in test_df.columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab3061d0-c136-40a8-abf1-ed8b48cdf34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2\n",
    "NewColumns=(column.replace(' ', '__') for column in renamed_df.columns)\n",
    "new_df = renamed_df.toDF(*NewColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38c2aa90-706a-4ab7-8e08-c3ec30f923dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = new_df.toDF(*(column.replace('__', '_') for column in new_df.columns))\n",
    "# new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9900550-eaa8-4637-ae27-ad5e51c20030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>updated_er_id</th>\n",
       "      <th>name</th>\n",
       "      <th>relationship_with_crd_owner</th>\n",
       "      <th>father_name</th>\n",
       "      <th>mother_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>member_id</th>\n",
       "      <th>address</th>\n",
       "      <th>state_name</th>\n",
       "      <th>district_name</th>\n",
       "      <th>rationcard_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122311</td>\n",
       "      <td>Aeeni Afridi</td>\n",
       "      <td>self</td>\n",
       "      <td>Saadiq Bidhar</td>\n",
       "      <td>Mausami De Bidhar</td>\n",
       "      <td>F</td>\n",
       "      <td>12/01/1960</td>\n",
       "      <td>60</td>\n",
       "      <td>54910117654401</td>\n",
       "      <td>Ekaparnika, Akshay Street, Sector 13, Nr. Rabp...</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>Thrissur</td>\n",
       "      <td>549101176544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112311</td>\n",
       "      <td>Aphridi Bhai</td>\n",
       "      <td>husband</td>\n",
       "      <td>Kanive Sangdo</td>\n",
       "      <td>Aadab Prabha Ganti</td>\n",
       "      <td>M</td>\n",
       "      <td>07/07/1963</td>\n",
       "      <td>57</td>\n",
       "      <td>54910117654402</td>\n",
       "      <td>Ekaparnika, Akshay Street, Sector 13, Nr. Rabp...</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>Thrissur</td>\n",
       "      <td>549101176544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2311</td>\n",
       "      <td>Faria Kanive</td>\n",
       "      <td>daughter</td>\n",
       "      <td>Afridi Bhai</td>\n",
       "      <td>Aeeni Afridi</td>\n",
       "      <td>F</td>\n",
       "      <td>19/10/1989</td>\n",
       "      <td>31</td>\n",
       "      <td>54910117654403</td>\n",
       "      <td>Ekaparnika, Akshay Street, Sector 13, Nr. Rabp...</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>Thrissur</td>\n",
       "      <td>549101176544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1502311</td>\n",
       "      <td>Bhupinder Lata Mattathil</td>\n",
       "      <td>daughter</td>\n",
       "      <td>Aphridi Bhai</td>\n",
       "      <td>Aini Afridi</td>\n",
       "      <td>F</td>\n",
       "      <td>07/12/1994</td>\n",
       "      <td>26</td>\n",
       "      <td>54910117654404</td>\n",
       "      <td>Ekaparnika, Akshay Street, Sector 13, Nr. Rabp...</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>Thrissur</td>\n",
       "      <td>549101176544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12112312</td>\n",
       "      <td>Suprabha Shri Mankad</td>\n",
       "      <td>self</td>\n",
       "      <td>Maanik Gupta Mankad</td>\n",
       "      <td>Ekta</td>\n",
       "      <td>F</td>\n",
       "      <td>10/02/1962</td>\n",
       "      <td>58</td>\n",
       "      <td>25516389278801</td>\n",
       "      <td>Kanchan, Bipasha Lane, Sector Number- 50, Opp....</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>East Godavari</td>\n",
       "      <td>255163892788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   updated_er_id                      name relationship_with_crd_owner  \\\n",
       "0         122311              Aeeni Afridi                        self   \n",
       "1         112311              Aphridi Bhai                     husband   \n",
       "2           2311              Faria Kanive                    daughter   \n",
       "3        1502311  Bhupinder Lata Mattathil                    daughter   \n",
       "4       12112312      Suprabha Shri Mankad                        self   \n",
       "\n",
       "           father_name         mother_name gender         dob  age  \\\n",
       "0        Saadiq Bidhar   Mausami De Bidhar      F  12/01/1960   60   \n",
       "1        Kanive Sangdo  Aadab Prabha Ganti      M  07/07/1963   57   \n",
       "2          Afridi Bhai        Aeeni Afridi      F  19/10/1989   31   \n",
       "3         Aphridi Bhai         Aini Afridi      F  07/12/1994   26   \n",
       "4  Maanik Gupta Mankad                Ekta      F  10/02/1962   58   \n",
       "\n",
       "        member_id                                            address  \\\n",
       "0  54910117654401  Ekaparnika, Akshay Street, Sector 13, Nr. Rabp...   \n",
       "1  54910117654402  Ekaparnika, Akshay Street, Sector 13, Nr. Rabp...   \n",
       "2  54910117654403  Ekaparnika, Akshay Street, Sector 13, Nr. Rabp...   \n",
       "3  54910117654404  Ekaparnika, Akshay Street, Sector 13, Nr. Rabp...   \n",
       "4  25516389278801  Kanchan, Bipasha Lane, Sector Number- 50, Opp....   \n",
       "\n",
       "       state_name  district_name  rationcard_id  \n",
       "0          Kerala       Thrissur   549101176544  \n",
       "1          Kerala       Thrissur   549101176544  \n",
       "2          Kerala       Thrissur   549101176544  \n",
       "3          Kerala       Thrissur   549101176544  \n",
       "4  Andhra Pradesh  East Godavari   255163892788  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"file:///home/ns22/Desktop/rationcard/batch_1_500000_entities_23_dec_2021_rationcard.csv\", sep=\",\")\n",
    "df.head(5)\n",
    "\n",
    "# in pandas \n",
    "# df.columns = [x.lower().replace(\"_\",\" \") for x in df.columns ] \n",
    "\n",
    "\n",
    "# in pyspark dataframe\n",
    "#new_df = df.toDF(*(x.lower().replace(\"_\",\" \") for x in df.columns))\n",
    "#new_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d645e-0d43-4303-a445-19fccb5820c4",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Parquet files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb607f-a8f3-417c-af9a-b9b12f6509ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Loading Data Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7b133a7-0dec-4677-bc2d-2b83dc19fc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(\"file:///home/ns22/Desktop/rationcard/batch_1_500000_entities_23_dec_2021_rationcard.csv\",header=True, sep=\",\")\n",
    "df = df.coalesce(1)\n",
    "df.write.parquet(\"file:///home/ns22/Desktop/temp/test_parquet_ration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ed1a7e0-d1b7-4dde-93ba-8069a4f2e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = spark.read.parquet(\"file:///home/ns22/Desktop/temp/test_parquet_ration.parquet\")\n",
    "parquet_file = parquet_file.coalesce(1)\n",
    "parquet_file.createOrReplaceGlobalTempView(\"parquetFile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87d73a30-a5e6-4fa3-8c6d-80a48a7dfd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+---------------------------+-------------------+------------------+------+----------+---+--------------+--------------------+--------------+-------------+-------------+\n",
      "|updated_er_id|                name|relationship_with_crd_owner|        father_name|       mother_name|gender|       dob|age|     member_id|             address|    state_name|district_name|rationcard_id|\n",
      "+-------------+--------------------+---------------------------+-------------------+------------------+------+----------+---+--------------+--------------------+--------------+-------------+-------------+\n",
      "|       122311|        Aeeni Afridi|                       self|      Saadiq Bidhar| Mausami De Bidhar|     F|12/01/1960| 60|54910117654401|Ekaparnika, Aksha...|        Kerala|     Thrissur| 549101176544|\n",
      "|       112311|        Aphridi Bhai|                    husband|      Kanive Sangdo|Aadab Prabha Ganti|     M|07/07/1963| 57|54910117654402|Ekaparnika, Aksha...|        Kerala|     Thrissur| 549101176544|\n",
      "|         2311|        Faria Kanive|                   daughter|        Afridi Bhai|      Aeeni Afridi|     F|19/10/1989| 31|54910117654403|Ekaparnika, Aksha...|        Kerala|     Thrissur| 549101176544|\n",
      "|      1502311|Bhupinder Lata Ma...|                   daughter|       Aphridi Bhai|       Aini Afridi|     F|07/12/1994| 26|54910117654404|Ekaparnika, Aksha...|        Kerala|     Thrissur| 549101176544|\n",
      "|     12112312|Suprabha Shri Mankad|                       self|Maanik Gupta Mankad|              Ekta|     F|10/02/1962| 58|25516389278801|Kanchan, Bipasha ...|Andhra Pradesh|East Godavari| 255163892788|\n",
      "+-------------+--------------------+---------------------------+-------------------+------------------+------+----------+---+--------------+--------------------+--------------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquet_file.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "574c605e-127d-445c-b7db-090c61735345",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file.createOrReplaceTempView(\"parquetFile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5297af6e-9f5e-4081-be0e-7be898b8843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT /*+ COALESCE(3) */ * from parquetFile\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66c2c1fe-0087-4a06-a7e7-4599ea903cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT /*+ REPARTITION(2, gender) */ * from parquetFile\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99c376-f41c-4f72-8680-bcea7cc3b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---+--------------+\n",
      "|                name|gender|age|    state_name|\n",
      "+--------------------+------+---+--------------+\n",
      "|        Aeeni Afridi|     F| 60|        Kerala|\n",
      "|        Aphridi Bhai|     M| 57|        Kerala|\n",
      "|        Faria Kanive|     F| 31|        Kerala|\n",
      "|Bhupinder Lata Ma...|     F| 26|        Kerala|\n",
      "|Suprabha Shri Mankad|     F| 58|Andhra Pradesh|\n",
      "+--------------------+------+---+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_states = spark.sql(\"select name, gender, age, state_name from parquetFile\")\n",
    "all_states.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b408c94-eb95-4c63-abfe-6d39d8553004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd4aa900-c388-4d7f-a789-29f4f7911d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea5d642d-1e56-4d01-98f9-3566524d00cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquet_file.repartition(4,col(\"state_name\")).write.partitionBy(col(\"state_name\")).mode(SaveMode.Append).saveAsTable(\"all_states_repartitioned_table\")\n",
    "\n",
    "#TypeError: Column is not iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86ad5fe1-4b88-4380-b749-366f4a69fea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|      state_name|Number_of_person|\n",
      "+----------------+----------------+\n",
      "|      Tamil Nadu|           90442|\n",
      "|   Uttar Pradesh|           72010|\n",
      "|     Maharashtra|           69190|\n",
      "|          Kerala|           62683|\n",
      "|       Karnataka|           59416|\n",
      "|  Andhra Pradesh|           54063|\n",
      "|     West Bengal|           52792|\n",
      "|         Gujarat|           45334|\n",
      "|       Rajasthan|           43270|\n",
      "|          Odisha|           40026|\n",
      "|           Bihar|           37998|\n",
      "|  Madhya Pradesh|           33918|\n",
      "|       Telangana|           29198|\n",
      "|           Assam|           24973|\n",
      "|          Punjab|           23385|\n",
      "|Himachal Pradesh|           18863|\n",
      "|       Jharkhand|           16937|\n",
      "|         Haryana|           13254|\n",
      "|     Uttarakhand|           12591|\n",
      "|     Chattisgarh|           11481|\n",
      "+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        state_name, \n",
    "        count(*) AS Number_of_person \n",
    "    FROM parquetFile \n",
    "    GROUP BY state_name \n",
    "    ORDER BY Number_of_person DESC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbd4cc0-68a2-4be9-ae08-99d13a1d19fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Compress further from parquet to snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bce632e-4b22-492f-9147-b78e9c14075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquet_file.write.option(\"compression\",\"snappy\").parquet(\"file:///home/ns22/Desktop/temp/snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "137e31cf-1e91-46df-b14c-f32b548d7691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15219768"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eef42b97-528f-45ec-8ff7-5f205d56fe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df = test_df.coalesce(1)\n",
    "test_df.write.option(\"compression\",\"snappy\").parquet(\"file:///home/ns22/Desktop/test_ration_parquet_new/snappy\")\n",
    "# More details about snappy compression\n",
    "# http://boristyukin.com/is-snappy-compressed-parquet-file-splittable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a17bf7a-c432-419d-994b-a5c36a439773",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_snappy_df = spark.read.parquet(\"file:///home/ns22/Desktop/test_ration_parquet_new/snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff8ea7b0-9492-4cee-8a62-26bbd62ed918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+---------------------------+--------------------+--------------------+------+----------+---+--------------+--------------------+----------+--------------+-------------+\n",
      "|updated_er_id|                name|relationship_with_crd_owner|         father_name|         mother_name|gender|       dob|age|     member_id|             address|state_name| district_name|rationcard_id|\n",
      "+-------------+--------------------+---------------------------+--------------------+--------------------+------+----------+---+--------------+--------------------+----------+--------------+-------------+\n",
      "|     12233168|              Iditri|                       self|         Yudhbir Ram|     Yudhbir Bibhuti|     F|07/04/1972| 48|39590910826901|X/61, 7 Floor, Va...| Jharkhand|West Singhbhum| 395909108269|\n",
      "|     11233168|    Yusuf Dasa Palit|                    husband|          Aamir Dasa|     Arwa Shri Aamir|     M|14/02/1981| 39|39590910826902|X/61, 7 Floor, Va...| Jharkhand|West Singhbhum| 395909108269|\n",
      "|  12112331149|     Kimatra Jawanda|                       self|          Ranjay Sah|   Taassar De Ranjay|     F|17/12/1953| 67|36968536578801|P 48, 2 Floor, An...| Jharkhand|        Bokaro| 369685365788|\n",
      "|  12112331287|      Dagang Mallick|                       self|Bhasker Olakkappadil|           Kaamda De|     F|04/01/1949| 71|92017688033301|I/194, 3 Floor, N...| Jharkhand|        Bokaro| 920176880333|\n",
      "|  11112331287|  Khalil Ish Mallick|                    husband|     Chaitanya Singh|       Milichaitanya|     M|04/05/1946| 74|92017688033302|I/194, 3 Floor, N...| Jharkhand|        Bokaro| 920176880333|\n",
      "|    112331287|                null|                        son|  Khalil Ish Mallick|       Dagangmallick|     M|04/09/1974| 46|92017688033303|I/194, 3 Floor, N...| Jharkhand|        Bokaro| 920176880333|\n",
      "|   1502331287|         Lila Nagpal|              granddaughter|                null|   Prema Ben Mallick|     F|18/12/2007| 13|92017688033304|I/194, 3 Floor, N...| Jharkhand|        Bokaro| 920176880333|\n",
      "|  12112331303|          Shanta Mul|                       self|    Bishwaji Jambagi|      Gulnar Jambagi|     F|11/06/1930| 90|52337276053301|Q/8, 1 Floor, Kum...| Jharkhand|       Latehar| 523372760533|\n",
      "|  11112331303|           Mul Raman|                    husband|         Tarbez Kaki|           Muna Kaki|     M|24/06/1931| 89|52337276053302|Q/8, 1 Floor, Kum...| Jharkhand|       Latehar| 523372760533|\n",
      "|    122331303|   Ben Udyati Mustaq|            daughter_in_law|          Ujjair Sah|Phassaang Vati Uj...|     F|05/09/1962| 58|52337276053303|Q/8, 1 Floor, Kum...| Jharkhand|       Latehar| 523372760533|\n",
      "|  12112331440|       Trishala Mati|                       self|Prathu Nandi Kakk...|               Sajni|     F|11/09/1943| 77|85052982958501|H 23, Hari City, ...| Jharkhand|East Singhbhum| 850529829585|\n",
      "|   1302331440|           Tejas Rai|            grandson_in_law|     Laliya Upadhyay|              Binati|     M|21/03/1988| 32|85052982958502|H 23, Hari City, ...| Jharkhand|East Singhbhum| 850529829585|\n",
      "|  12112331544|     Gauravi Vishaal|                       self|Gyanender Bhaai Kaur|  Bhanupreeya Prabha|     F|07/04/1970| 50|23310704828201|F-106, 5 Floor, T...| Jharkhand|        Bokaro| 233107048282|\n",
      "|    122331660|Shivanya Kumari H...|                       self|Neej Ishwar Sreed...|Aamaal Shri Sreed...|     F|26/12/1982| 38|95604444306501|T 65, 3 Floor, Vi...| Jharkhand|       Dhanbad| 956044443065|\n",
      "|    112331660|              Hardev|                    husband|Rajmani Nochooruv...|Nochooruvalappil ...|     M|07/09/1981| 39|95604444306502|T 65, 3 Floor, Vi...| Jharkhand|       Dhanbad| 956044443065|\n",
      "|      2331660|Babul Prakash Noc...|                        son|              Hardev|     Shivanya Hardev|     M|21/01/2005| 15|95604444306503|T 65, 3 Floor, Vi...| Jharkhand|       Dhanbad| 956044443065|\n",
      "|    122331709|     Jaswanti Prabha|                       self|         Sohel Palam|          Harhsa Ben|     F|20/07/1986| 34|82196926479001|Oaklands, Mahroj ...| Jharkhand|East Singhbhum| 821969264790|\n",
      "|  12112331709|       Horumai Katka|              mother_in_law|               Nipun|          Naaz Katka|     F|11/01/1960| 60|82196926479002|Oaklands, Mahroj ...| Jharkhand|East Singhbhum| 821969264790|\n",
      "|  12112331732|Gollo Kumari Pall...|                       self|Shadan Pallippara...|Ngurang Pallippar...|     F|06/11/1935| 85|26806460204301|Hastinapuri, Vije...| Jharkhand|         Dumka| 268064602043|\n",
      "|  11112331732|Jeetram Sah Palli...|                    husband|Aanand Pallippara...|Bandana Kumari Pa...|     M|07/07/1941| 79|26806460204302|Hastinapuri, Vije...| Jharkhand|         Dumka| 268064602043|\n",
      "+-------------+--------------------+---------------------------+--------------------+--------------------+------+----------+---+--------------+--------------------+----------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_snappy_df.filter(test_snappy_df.state_name == \"Jharkhand\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25beef04-f212-4751-a937-b8d3a3dcc7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fcd70-ff4c-452a-9e08-4bf605e625e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c55c5c30-27e2-4a07-ac35-ef218536edf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Read ElasticSearch index into pyspark dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a6aa68-23b5-4819-978f-49a755d5505e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/11 11:26:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "import elasticsearch \n",
    "from elasticsearch.helpers import scan\n",
    "import pandas as pd\n",
    "# import elastic_mapping\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"elastc_test\").getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2e5a6012-2e36-4386-9b60-e3adeba4b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-8.3.1-py3-none-any.whl (382 kB)\n",
      "\u001b[K     |ââââââââââââââââââââââââââââââââ| 382 kB 214 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting elastic-transport<9,>=8\n",
      "  Using cached elastic_transport-8.1.2-py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: urllib3<2,>=1.26.2 in /home/ns22/anaconda3/lib/python3.9/site-packages (from elastic-transport<9,>=8->elasticsearch) (1.26.7)\n",
      "Requirement already satisfied: certifi in /home/ns22/anaconda3/lib/python3.9/site-packages (from elastic-transport<9,>=8->elasticsearch) (2021.10.8)\n",
      "Installing collected packages: elastic-transport, elasticsearch\n",
      "Successfully installed elastic-transport-8.1.2 elasticsearch-8.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5753fe-3886-4f8c-8938-f676464b56cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "\n",
    "\n",
    "es = elasticsearch.Elasticsearch([{'host': '192.168.181.124', 'port': 9200, 'scheme': 'http'}], max_retries=10,\n",
    "                                 retry_on_timeout=True)\n",
    "elastic_index_name = \"eva_trends_trial\"\n",
    "\n",
    "# Elastic Connection\n",
    "def elastic_connection():\n",
    "    if es.ping():\n",
    "        print(\"ElasticSearch Connect Successfully\")\n",
    "    else:\n",
    "        print('Awww it could not connect!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914ee503-76c4-490d-8f29-238d4172664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticSearch Connect Successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_305380/65089427.py:10: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  if es.ping():\n"
     ]
    }
   ],
   "source": [
    "elastic_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a94346-58c3-49da-99ea-eb8919b4ad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_305380/535923404.py:29: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  result = list(rel)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of documents:  184957\n",
      "Data Fetched\n",
      "Completed\n",
      "['articletime', 'baseurl', 'count_int', 'count_str', 'country', 'hostname', 'ingestedtime', 'language', 'rank', 'resource_type', 'title', 'category', 'image_link', 'news_source', 'related_links']\n",
      "+--------------------+--------------------+---------+---------+--------+--------------------+\n",
      "|         articletime|             baseurl|count_int|count_str| country|            hostname|\n",
      "+--------------------+--------------------+---------+---------+--------+--------------------+\n",
      "|2022-07-05T15:04:25Z|https://trends24.in/|        0|       NA|Pakistan|            Trends24|\n",
      "|2022-07-05T17:56:04Z|https://trends.go...|     null|     null|   India|Google Realtime T...|\n",
      "|2022-07-05T15:04:20Z|https://trends24.in/|        0|       NA|Pakistan|            Trends24|\n",
      "|2022-07-05T15:03:56Z|https://trends24.in/|    43000|      43K|Pakistan|            Trends24|\n",
      "|2022-07-05T19:56:04Z|https://trends.go...|     null|     null|   India|Google Realtime T...|\n",
      "|2022-07-05T15:04:25Z|https://trends24.in/|        0|       NA|Pakistan|            Trends24|\n",
      "|2022-07-05T09:56:04Z|https://trends.go...|     null|     null|   India|Google Realtime T...|\n",
      "|2022-07-05T15:04:25Z|https://trends24.in/|        0|       NA|Pakistan|            Trends24|\n",
      "|2022-07-05T18:56:04Z|https://trends.go...|     null|     null|   India|Google Realtime T...|\n",
      "|2022-07-05T15:03:56Z|https://trends24.in/|    11000|      11K|Pakistan|            Trends24|\n",
      "+--------------------+--------------------+---------+---------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def get_query(size):\n",
    "    \n",
    "    query = {\n",
    "            \"query\": {\n",
    "                \"match_all\": {}\n",
    "                },\n",
    "\t\t    \"sort\": [\n",
    "\t\t    {\n",
    "\t\t      \"ingestedtime\": {\n",
    "\t\t\t        \"order\": \"desc\"\n",
    "\t\t      \t\t}\n",
    "    \t\t\t}\n",
    "  \t\t    ]\n",
    "        }\n",
    "\n",
    "\n",
    "    rel = scan(client=es,             \n",
    "                query=query,\n",
    "               size=100,\n",
    "                scroll='1m',\n",
    "                index=elastic_index_name,\n",
    "                raise_on_error=True,\n",
    "                preserve_order=False,\n",
    "                clear_scroll=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Keep response in a list.\n",
    "    result = list(rel)\n",
    "    length = len(result)\n",
    "    doc_count = {}\n",
    "    # doc_count['total_count'] = length\n",
    "    print(\"count of documents: \", length)\n",
    "    temp = []\n",
    "    # temp.append(doc_count)\n",
    "    # We need only '_source', which has all the fields required.\n",
    "    # This elimantes the elasticsearch metdata like _id, _type, _index.\n",
    "    for hit in result:\n",
    "        temp.append(hit['_source'])\n",
    "        if len(temp) > size:\n",
    "            # print(\"temp : \" ,temp)\n",
    "            return temp   \n",
    "    return temp\n",
    "\n",
    "\n",
    "\n",
    "data = get_query(100)\n",
    "print(\"Data Fetched\")\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "print(\"Completed\")\n",
    "print(df.columns)\n",
    "print(df.select('articletime', 'baseurl', 'count_int', 'count_str', 'country', 'hostname').show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a9bfb-4430-4684-9931-e44e852e3698",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Modified..Read Elastic search index data in pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "64caa9f9-dc82-4456-8c0e-46200c28cd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of documents:  10000\n",
      "Data Fetched\n",
      "Completed\n",
      "['articletime', 'baseurl', 'count_int', 'count_str', 'country', 'hostname', 'ingestedtime', 'language', 'rank', 'resource_type', 'title']\n",
      "+--------------------+--------------------+---------+---------+--------------+--------+\n",
      "|         articletime|             baseurl|count_int|count_str|       country|hostname|\n",
      "+--------------------+--------------------+---------+---------+--------------+--------+\n",
      "|                null|                null|     null|     null|          null|    null|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|   789000|     789K|United Kingdom|Trends24|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|   110000|     110K|United Kingdom|Trends24|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|    88000|      88K|United Kingdom|Trends24|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|    77000|      77K|United Kingdom|Trends24|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|    51000|      51K|United Kingdom|Trends24|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|   267000|     267K|United Kingdom|Trends24|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|    37000|      37K|United Kingdom|Trends24|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|    10000|      10K|United Kingdom|Trends24|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|    60000|      60K|United Kingdom|Trends24|\n",
      "+--------------------+--------------------+---------+---------+--------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_305380/4220753924.py:17: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  rel = es.search(index=elastic_index_name,body=query,size=11)\n",
      "/tmp/ipykernel_305380/4220753924.py:17: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  rel = es.search(index=elastic_index_name,body=query,size=11)\n"
     ]
    }
   ],
   "source": [
    "def get_query():\n",
    "    \n",
    "    query = {\n",
    "            \"query\": {\n",
    "                \"match_all\": {}\n",
    "                },\n",
    "\t\t    \"sort\": [\n",
    "\t\t    {\n",
    "\t\t      \"ingestedtime\": {\n",
    "\t\t\t        \"order\": \"desc\"\n",
    "\t\t      \t\t}\n",
    "    \t\t\t}\n",
    "  \t\t    ]\n",
    "        }\n",
    "\n",
    "\n",
    "    rel = es.search(index=elastic_index_name,body=query,size=11)\n",
    "    \n",
    "\n",
    "    length = rel['hits']['total']['value']\n",
    "    # Keep response in a list.\n",
    "    # result = list(rel)\n",
    "    # length = len(result)\n",
    "    doc_count = {}\n",
    "    # doc_count['total_count'] = length\n",
    "    print(\"count of documents: \", length)\n",
    "    temp = []\n",
    "    temp.append(doc_count)\n",
    "    # We need only '_source', which has all the fields required.\n",
    "    # This elimantes the elasticsearch metdata like _id, _type, _index.\n",
    "    for hit in rel['hits']['hits']:\n",
    "        temp.append(hit['_source'])   \n",
    "    return temp\n",
    "\n",
    "\n",
    "\n",
    "data = get_query()\n",
    "print(\"Data Fetched\")\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "print(\"Completed\")\n",
    "print(df.columns)\n",
    "print(df.select('articletime', 'baseurl', 'count_int', 'count_str', 'country', 'hostname').show(10))\n",
    "# columns = df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ebb5b023-482b-4003-8242-1c0ad20a6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletes the record having all null values\n",
    "#df.na.drop(\"all\").show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "93429192-0bce-471c-b9c5-6c470d8b7298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+---------+--------------+--------+--------------------+--------+----+-------------+--------------+\n",
      "|         articletime|             baseurl|count_int|count_str|       country|hostname|        ingestedtime|language|rank|resource_type|         title|\n",
      "+--------------------+--------------------+---------+---------+--------------+--------+--------------------+--------+----+-------------+--------------+\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/| 789000.0|     789K|United Kingdom|Trends24|2022-07-07T17:38:31Z|      en| 1.0|       Trends|         Boris|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/| 110000.0|     110K|United Kingdom|Trends24|2022-07-07T17:38:31Z|      sl| 2.0|       Trends|          Gove|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|  88000.0|      88K|United Kingdom|Trends24|2022-07-07T17:38:31Z|      tl| 3.0|       Trends|         #PMQs|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|  77000.0|      77K|United Kingdom|Trends24|2022-07-07T17:38:31Z|      en| 4.0|       Trends| #itshappening|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|  51000.0|      51K|United Kingdom|Trends24|2022-07-07T17:38:31Z|      fr| 5.0|       Trends|       Jacques|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/| 267000.0|     267K|United Kingdom|Trends24|2022-07-07T17:38:31Z|      de| 6.0|       Trends|Prime Minister|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|  37000.0|      37K|United Kingdom|Trends24|2022-07-07T17:38:31Z|      so| 7.0|       Trends|    #WEURO2022|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|  10000.0|      10K|United Kingdom|Trends24|2022-07-07T17:38:31Z|      fr| 8.0|       Trends|    #Lionesses|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|  60000.0|      60K|United Kingdom|Trends24|2022-07-07T17:38:31Z|      en| 9.0|       Trends|Downing Street|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|      0.0|       NA|United Kingdom|Trends24|2022-07-07T17:38:31Z|      en|10.0|       Trends| #ToryMeltdown|\n",
      "|2022-07-07T12:38:31Z|https://trends24.in/|      0.0|       NA|United Kingdom|Trends24|2022-07-07T17:38:31Z|      de|11.0|       Trends| Brandon Lewis|\n",
      "+--------------------+--------------------+---------+---------+--------------+--------+--------------------+--------+----+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert pyspark df to pandas df , use iloc and return to pyspark df\n",
    "ps_df = df.toPandas()\n",
    "new_df = ps_df.iloc[1:]\n",
    "sparkDF=spark.createDataFrame(new_df)\n",
    "sparkDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92a96b81-89e8-492a-993d-102fa3734b65",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nSyntax error at or near 'TOP': missing 'FROM'(line 1, pos 7)\n\n== SQL ==\nDELETE TOP(1) FROM my_table\n-------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_305380/928824680.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# df.createOrReplaceTempView(\"my_table\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# spark.sql(\"select * from my_table limit 5\").show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DELETE TOP(1) FROM my_table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \nSyntax error at or near 'TOP': missing 'FROM'(line 1, pos 7)\n\n== SQL ==\nDELETE TOP(1) FROM my_table\n-------^^^\n"
     ]
    }
   ],
   "source": [
    "# df.createOrReplaceTempView(\"my_table\")\n",
    "# spark.sql(\"select * from my_table limit 5\").show()\n",
    "# spark.sql(\"DELETE TOP(1) FROM my_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec398da2-b40d-4a49-866d-5d1b51f6daf6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Mapping in Spark using broadcast variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4628f866-3f6e-4c6a-81a2-d76458f78945",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## PySpark RDD Broadcast variable example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c91172d2-baf8-4a32-906a-4b07d82826d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/08 14:30:21 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ce4ed-9ea5-4d9f-876a-c5d01f240302",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## PySpark DataFrame Broadcast variable example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d11a8019-7e2a-4aec-9a7c-ffc518f7ce95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|state     |\n",
      "+---------+--------+-------+----------+\n",
      "|James    |Smith   |USA    |California|\n",
      "|Michael  |Rose    |USA    |New York  |\n",
      "|Robert   |Williams|USA    |California|\n",
      "|Maria    |Jones   |USA    |Florida   |\n",
      "+---------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc68e167-5458-474d-bd0e-4ab35cd217e0",
   "metadata": {},
   "source": [
    "# SQL in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "81c1e60d-1dee-4de8-803a-4bc25d2a48c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database new_db;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c5960dc-df2c-45f0-850c-b5fa6070e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# CREATE TABLE CUSTOMERS(\n",
    "#    ID   INT              NOT NULL,\n",
    "#    NAME VARCHAR (20)     NOT NULL,\n",
    "#    AGE  INT              NOT NULL,\n",
    "#    ADDRESS  CHAR (25) ,\n",
    "#    SALARY   DECIMAL (18, 2)\n",
    "# );\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6bf81ed6-cd17-4e20-8220-8edcc4f1f433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|   new_db|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d1886ef9-4122-438e-b1e3-d43bfbc4566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = spark.read.parquet(\"file:///home/ns22/Desktop/temp/test_parquet_ration.parquet\")\n",
    "test_file = test_file.coalesce(1)\n",
    "test_file.createOrReplaceTempView(\"testTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7e39df8f-acd1-4f34-8da7-4c4f22842b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+---------------------------+-------------+-----------------+------+----------+---+--------------+--------------------+----------+-------------+-------------+\n",
      "|updated_er_id|        name|relationship_with_crd_owner|  father_name|      mother_name|gender|       dob|age|     member_id|             address|state_name|district_name|rationcard_id|\n",
      "+-------------+------------+---------------------------+-------------+-----------------+------+----------+---+--------------+--------------------+----------+-------------+-------------+\n",
      "|       122311|Aeeni Afridi|                       self|Saadiq Bidhar|Mausami De Bidhar|     F|12/01/1960| 60|54910117654401|Ekaparnika, Aksha...|    Kerala|     Thrissur| 549101176544|\n",
      "+-------------+------------+---------------------------+-------------+-----------------+------+----------+---+--------------+--------------------+----------+-------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM testTable\n",
    "\"\"\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3ea8ef38-75f8-4cf3-8c20-5025d6d25fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|updated_er_id|\n",
      "+-------------+\n",
      "|       122311|\n",
      "|       112311|\n",
      "|         2311|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT updated_er_id FROM testTable limit 3 \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "df425d94-0581-4671-9faa-0a76461abbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|     F|  464297|\n",
      "|     M|  384088|\n",
      "|     T|     438|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT gender, count(*) FROM testTable GROUP BY gender\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a245c2-b883-4e28-939d-338d97e5f53d",
   "metadata": {},
   "source": [
    "# Print All available dataframes in current spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "532af650-e24d-4851-ba43-4b07d9827276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__', '___', 'df', 'parquet_file', 'all_states', 'sparkDF', '_87', '_89', '_91', 'test_file']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "print([k for (k,v) in globals().items() if isinstance(v, DataFrame)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "668091dc-867e-4096-abac-9b426cd3cb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/home/ns22/Downloads/new_validation/spark-warehouse'),\n",
       " Database(name='new_db', description='', locationUri='file:/home/ns22/Downloads/new_validation/spark-warehouse/new_db.db')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
